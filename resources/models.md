##  A brief Description of the Models we used
##

### Logistic Regression
###
This type of statistical model (also known as logit model) is often used for classification and predictive analytics. Logistic regression estimates the probability of an event occurring, such as voted or didn't vote, based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1 (binary). In logistic regression, a logit transformation is applied on the odds, that is the probability of success divided by the probability of failure.
###

### XGBoost
###
XGBoost stands for 'Extreme Gradient Boosting'. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements Machine Learning algorithms under the Gradient Boosting framework. It provides a parallel tree boosting to solve many data science problems in a fast and accurate way. It can be used in both regression(numerical) and classification(categorical) problems.
###

### Naive Bayes
###
Naive Bayes is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems.
It is mainly used in text classification that includes a high-dimensional training dataset.
Naive Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.
###

### Random Forest
###
The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. It's a powerful and versatile supervised machine learning algorithm that grows and combines multiple decision trees to create a 'forest'. It can be used for both classification and regression problems.
###

### K-Nearest Neighbours (KNN)
###
K-Nearest Neighbors is a machine learning technique and algorithm that can be used for both regression and classification tasks. K-Nearest Neighbors examines the labels of a chosen number of data points surrounding a target data point, in order to make a prediction about the class that the data point falls into.
###

###
###
###
###
hint::  More models will be added in the next app update :sunglasses:\
(:muscle:the ability to classify multiple tweets at once will also be added!)
